{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOI7P0MIUcAK",
        "outputId": "12819d22-f6de-45e9-bd54-a588470c2815"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why do we need text summarization?\n",
        "Propelled by the modern technological innovations, data is to this century what oil was to the previous one. Today, our world is parachuted by the gathering and disseminationof huge amounts of data. In fact, the International Data Corporation (IDC) projects that the total amount ofdigital data circulating annually around the world would sprout from 4.4 zettabytes in2013 to hit 180 zettabytes in 2025. That’s a lot of data! With such a big amount of data circulating in the digital space, there is need to developmachine learning algorithms that can automatically shorten longer texts and deliveraccurate summaries that can fluently pass the intended messages. Furthermore, applying text summarization reduces reading time, accelerates the processof researching for information, and increases the amount of information that can fit inan area.(Ref: A Quick Introduction to Text Summarization in Machine Learning | by Dr. Michael J. Garbade | Towards Data Science)\n",
        "\n",
        "\n",
        "Types of text summarization\n",
        "Based on the input document There are mainly four types of summaries:\n",
        "\n",
        "Single Document Summary: Summary of a Single Document\n",
        "Multi-Document Summary: Summary from multiple documents\n",
        "Query Focused Summary: Summary of a specific query\n",
        "Informative Summary: It includes a summary of the full information.\n",
        "ref:https://towardsdatascience.com/understanding-automatic-text-summarization-1-extractive-methods-8eb512b21ecc\n",
        "\n",
        "Based on output of the summary produced could be an extractive or abstractive summary.\n",
        "\n",
        "Extractive method\n",
        "Selects the n numbers of the most important sentences from the article which convey the main idea of the article.\n",
        "\n",
        "Abstractive method\n",
        "This method uses concepts of deep learning like encoder-decoder architecture, LSTM(Long Short Term Memory) networks. As a reult of paraphrasing the text, this method may sometimes produse sentences that dont make sense.\n",
        "\n",
        "How does an extractive text summarization algorithm work?\n",
        "(Ref:https://pkghosh.wordpress.com/2019/06/27/six-unsupervised-extractive-text-summarization-techniques-side-by-side/)\n",
        "\n",
        "Typically, extractive-based approaches summarize texts by:\n",
        "Spliting the document into sentences\n",
        "Pre-processing each sentence to remove noise.This includes tokenization, stop word removal, and lemmatization\n",
        "Assigning algorithmic specific score to each sentence\n",
        "Sorting the sentences in descending order of the score to retain the top n, where n is a configurable parameter.\n",
        "combining the top n sentences to form the summary.\n",
        "All these algorithms follow the steps mentioned above.However,the scoring logic is different for each algorithm.\n",
        "\n",
        "Diversity\n",
        "(Ref:https://pkghosh.wordpress.com/2019/06/27/six-unsupervised-extractive-text-summarization-techniques-side-by-side/) Ideally, we want the sentences in the summary to represent the underlying concepts and and not have any redundancy. i.e.sentences should be as diverse as possible. Diversity is the opposite of similarity. Sum Basic, Latent semantic indexing and Non negative matrix factorization have redundancy reduction built into the algorithms.\n",
        "\n",
        "For the remaining algorithms, explicit diversification is necessary. A technique called Maximal Marginal Relevance (MMR) is used for these other algorithms, that require diversification as an extra step.\n",
        "\n",
        "The steps are as follows (Ref:https://pkghosh.wordpress.com/2019/06/27/six-unsupervised-extractive-text-summarization-techniques-side-by-side/)\n",
        "\n",
        "Get sentences sorted in descending score from the core algorithm\n",
        "To select a sentence find diversity with respect to already selected sentence. This could be minimum, maximum or average diversity with already selected sentences.\n",
        "An weighted score is found based on the original score and the diversity score\n",
        "The sentence with the highest weighted score is selected next\n",
        "Get sentences sorted in descending score from the core algorithm\n",
        "To select a sentence find diversity with respect to already selected sentence. This could be minimum, maximum or average diversity with already selected sentences.\n",
        "An weighted score is found based on the original score and the diversity score\n",
        "The sentence with the highest weighted score is selected next\n",
        "Summary Evaluation\n",
        "There are two types of evaluations:\n",
        "\n",
        "Human Evaluation\n",
        "Automatic Evaluation\n",
        "ROUGE: ROUGE stands for Recall-Oriented Understudy for Gisting Evaluation. It is the method that determines the quality of the summary by comparing it to other summaries made by humans as a reference. The intuition behind this is if a model creates a good summary, then it must have common overlapping portions with the human references.\n",
        "\n",
        "ROUGE-n: It is measure on the comparison between the machine-generated output and the reference output based on n-grams. An n-gram is a contiguous sequence of n items from a given sample of text or speech, i.e, it is simply a sequence of words. Bigrams mean two words, Trigrams mean 3 words and so on. We normally use Unigrams and Bigrams.\n",
        "ref:https://towardsdatascience.com/understanding-automatic-text-summarization-1-extractive-methods-8eb512b21ecc\n",
        "\n",
        "For this project, I use the Rouge metric to calculate the quality of the summary.\n",
        "Python ROUGE is a native python implementation of ROUGE, designed to replicate results from the original perl package.\n",
        "Ref:https://pypi.org/project/rouge-score/\n",
        "\n",
        "ROUGE for Python\n",
        "There are ROUGE implementations available for Python, however some are not native python due to their dependency on the perl script, and others provide differing results when compared with the original implementation. This makes it difficult to directly compare with known results.\n",
        "\n",
        "This package is designed to replicate perl results. It implements:\n",
        "\n",
        "ROUGE-N (N-gram) scoring ROUGE-L (Longest Common Subsequence) scoring\n",
        "Ref:https://pypi.org/project/rouge-score/\n",
        "\n",
        "For this project, I've explored the following modules and algorithms:\n",
        "\n",
        "Text Rank - Word embedding GLOVE, graph and Page Rank\n",
        "Gensim - Variation of Text Rank\n",
        "NLTK - Term frequency\n",
        "Spacy - Sci Spacy Word embedding and Term Frequency/Inverse Frequency algorithm\n",
        "BERT Sum -greedy algorithm\n",
        "1. TextRank\n",
        "In Text Rank, sentence term matrix is used to cosine similarity between sentences. The similarity matrix is used to construct a graph, where sentences are nodes. Text rank algorithm is run on the graph. Embedding Text Rank is similar, except that instead of raw sentence term vector, sentence embedding vector is used to calculate similarity matrix. Sentence embedding vector is calculated by averaging embeddings of all the words in the sentence.\n",
        "\n",
        "Code source: https://www.analyticsvidhya.com/blog/2018/11/introduction-text-summarization-textrank-python/"
      ],
      "metadata": {
        "id": "MKrOgfhcgEtM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('punkt') # one time execution\n",
        "import re\n",
        "#Import library\n",
        "from IPython.display import Image"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOcM133xUkft",
        "outputId": "a40652e3-0b06-4501-e68e-1a115bb52152"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "entire_df = pd.read_csv(\"/content/drive/MyDrive/medicine_articles.csv\")"
      ],
      "metadata": {
        "id": "rPonZzcIUtNH"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Generate a single article from all the articles. \n",
        "text=entire_df ['all_text'][1]\n",
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "fP0Ra5AYUxLo",
        "outputId": "633d0349-2900-47af-d241-696592d328cf"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"b'Long COVID is an often debilitating illness that occurs in at least 10% of severe acute respiratory syndrome coronavirus\\\\xc2\\\\xa02 (SARS-CoV-2) infections. More than 200 symptoms have been identified with impacts on multiple organ systems. At least 65 million individuals worldwide are estimated to have long COVID, with cases increasing daily. Biomedical research has made substantial progress in identifying various pathophysiological changes and risk factors and in characterizing the illness; further, similarities with other viral-onset illnesses such as myalgic encephalomyelitis/chronic fatigue syndrome and postural orthostatic tachycardia syndrome have laid the groundwork for research in the field. In this Review, we explore the current literature and highlight key findings, the overlap with other conditions, the variable onset of symptoms, long COVID in children and the impact of vaccinations. Although these key findings are critical to understanding long COVID, current diagnostic and treatment options are insufficient, and clinical trials must be prioritized that address leading hypotheses. Additionally, to strengthen long COVID research, future studies must account for biases and SARS-CoV-2 testing issues, build on viral-onset research, be inclusive of marginalized populations and meaningfully engage patients throughout the research process. None Long COVID is a multisystemic illness encompassing ME/CFS, dysautonomia, impacts on multiple organ systems, and vascular and clotting abnormalities. It has already debilitated millions of individuals worldwide, and that number is continuing to grow. On the basis of more than 2 years of research on long COVID and decades of research on conditions such as ME/CFS, a significant proportion of individuals with long COVID may have lifelong disabilities if no action is taken. Diagnostic and treatment options are currently insufficient, and many clinical trials are urgently needed to rigorously test treatments that address hypothesized underlying biological mechanisms, including viral persistence, neuroinflammation, excessive blood clotting and autoimmunity.'\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Split Text into Sentences\n",
        "# break the text into individual sentences. using the sent_tokenize( ) function of the nltk library to do this.\n",
        "#Flattening lists means converting a multidimensional or nested list into a one-dimensional list.\n",
        "from nltk.tokenize import sent_tokenize\n",
        "sentences= sent_tokenize(text)"
      ],
      "metadata": {
        "id": "BFQPSrTAVNiV"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Let’s print a few elements of the list sentences.\n",
        "\n",
        "sentences[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kggVwpnCVdrG",
        "outputId": "7b63260b-c095-47ea-c636-cf1dc8fcf6cf"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"b'Long COVID is an often debilitating illness that occurs in at least 10% of severe acute respiratory syndrome coronavirus\\\\xc2\\\\xa02 (SARS-CoV-2) infections.\",\n",
              " 'More than 200 symptoms have been identified with impacts on multiple organ systems.',\n",
              " 'At least 65 million individuals worldwide are estimated to have long COVID, with cases increasing daily.',\n",
              " 'Biomedical research has made substantial progress in identifying various pathophysiological changes and risk factors and in characterizing the illness; further, similarities with other viral-onset illnesses such as myalgic encephalomyelitis/chronic fatigue syndrome and postural orthostatic tachycardia syndrome have laid the groundwork for research in the field.',\n",
              " 'In this Review, we explore the current literature and highlight key findings, the overlap with other conditions, the variable onset of symptoms, long COVID in children and the impact of vaccinations.']"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract word vectors\n",
        "#GloVe is a commonly used algorithm for natural language processing (NLP). It was trained on Wikipedia and Gigawords.\n",
        "#Get the word embeddings\n",
        "word_embeddings = {}\n",
        "f = open(\"/content/drive/MyDrive/archive/glove.6B.300d.txt\", encoding='utf-8')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    word_embeddings[word] = coefs\n",
        "f.close()"
      ],
      "metadata": {
        "id": "iSr0AFMEVjl1"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Getting the number of word embeddings in this glove model \n",
        "len(word_embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rh9NKMqV0CY",
        "outputId": "8deaeeed-ec3b-477b-dc95-097ab962579f"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "400000"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text Preprocessing\n",
        "It is always a good practice to make your textual data noise-free as much as possible. So, let’s do some basic text cleaning."
      ],
      "metadata": {
        "id": "CbUU1EcPhJZ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# remove punctuations, numbers and special characters\n",
        "clean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \")\n",
        "\n",
        "# make alphabets lowercase\n",
        "clean_sentences = [s.lower() for s in clean_sentences]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3-jd1m6V0fO",
        "outputId": "82c99d63-ec3b-40bb-f7ad-edefa8d94a0e"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-48-57e05bf8eb2b>:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  clean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Get rid of the stopwords (commonly used words of a language – is, am, the, of, in, etc.) present in the sentences. \n",
        "#If you have not downloaded nltk-stopwords, then execute the following line of code:\n",
        "\n",
        "nltk.download('stopwords')\n",
        "#Now we can import the stopwords.\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "#Let’s define a function to remove these stopwords from our dataset.\n",
        "\n",
        "# function to remove stopwords\n",
        "def remove_stopwords(sen):\n",
        "    sen_new = \" \".join([i for i in sen if i not in stop_words])\n",
        "    return sen_new\n",
        "# remove stopwords from the sentences\n",
        "clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]\n",
        "#We will use clean_sentences to create vectors for sentences in our data with the help of the GloVe word vectors."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFnn9qDVV4VR",
        "outputId": "4ccb1ab6-1774-4b67-887c-becc8980f2ab"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clean_sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7gofJbjV8Z7",
        "outputId": "55963c74-88a0-4000-e8ca-fd89b68195e5"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['b long covid often debilitating illness occurs least severe acute respiratory syndrome coronavirus xc xa sars cov infections',\n",
              " 'symptoms identified impacts multiple organ systems',\n",
              " 'least million individuals worldwide estimated long covid cases increasing daily',\n",
              " 'biomedical research made substantial progress identifying various pathophysiological changes risk factors characterizing illness similarities viral onset illnesses myalgic encephalomyelitis chronic fatigue syndrome postural orthostatic tachycardia syndrome laid groundwork research field',\n",
              " 'review explore current literature highlight key findings overlap conditions variable onset symptoms long covid children impact vaccinations',\n",
              " 'although key findings critical understanding long covid current diagnostic treatment options insufficient clinical trials must prioritized address leading hypotheses',\n",
              " 'additionally strengthen long covid research future studies must account biases sars cov testing issues build viral onset research inclusive marginalized populations meaningfully engage patients throughout research process',\n",
              " 'none long covid multisystemic illness encompassing cfs dysautonomia impacts multiple organ systems vascular clotting abnormalities',\n",
              " 'already debilitated millions individuals worldwide number continuing grow',\n",
              " 'basis years research long covid decades research conditions cfs significant proportion individuals long covid may lifelong disabilities action taken',\n",
              " 'diagnostic treatment options currently insufficient many clinical trials urgently needed rigorously test treatments address hypothesized underlying biological mechanisms including viral persistence neuroinflammation excessive blood clotting autoimmunity']"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vector Representation of Sentences"
      ],
      "metadata": {
        "id": "0EfKewjjhQby"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract word vectors\n",
        "word_embeddings = {}\n",
        "f = open(\"/content/drive/MyDrive/archive/glove.6B.300d.txt\", encoding='utf-8')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    word_embeddings[word] = coefs\n",
        "f.close()"
      ],
      "metadata": {
        "id": "tMe8KtI7WKwf"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Now, let’s create vectors for our sentences. \n",
        "#We will first fetch vectors (each of size 100 elements) for the constituent words in a sentence and then \n",
        "#take mean/average of those vectors to arrive at a consolidated vector for the sentence.\n",
        "\n",
        "sentence_vectors = []\n",
        "for i in clean_sentences:\n",
        "  if len(i) != 0:\n",
        "    v = sum([word_embeddings.get(w, np.zeros((300,))) for w in i.split()])/(len(i.split())+0.003)\n",
        "  else:\n",
        "    v = np.zeros((300,))\n",
        "  sentence_vectors.append(v)"
      ],
      "metadata": {
        "id": "womPjOj1WLry"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similarity Matrix Preparation"
      ],
      "metadata": {
        "id": "pGbVMqVyhWcP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# similarity matrix\n",
        "sim_mat = np.zeros([len(sentences), len(sentences)])\n",
        "#We will use Cosine Similarity to compute the similarity between a pair of sentences.\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "#And initialize the matrix with cosine similarity scores.\n",
        "\n",
        "for i in range(len(sentences)):\n",
        "  for j in range(len(sentences)):\n",
        "    if i != j:\n",
        "      sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,300), sentence_vectors[j].reshape(1,300))[0,0]\n",
        " "
      ],
      "metadata": {
        "id": "8u_EUx1zWPta"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Applying PageRank Algorithm\n",
        "Before proceeding further, let’s convert the similarity matrix sim_mat into a graph. The nodes of this graph will represent the sentences and the edges will represent the similarity scores between the sentences. On this graph, we will apply the PageRank algorithm to arrive at the sentence rankings."
      ],
      "metadata": {
        "id": "7tUyiOORheLZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "\n",
        "nx_graph = nx.from_numpy_array(sim_mat)\n",
        "scores = nx.pagerank(nx_graph)"
      ],
      "metadata": {
        "id": "7vu0LOKpWTcB"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary Extraction"
      ],
      "metadata": {
        "id": "cSHg36M2hiD3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Summary Extraction\n",
        "#Finally,  extract the top N sentences based on their rankings for summary generation.\n",
        "summary = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n",
        "# Extract no1 ranked sentences as the summary\n",
        "for i in range(1):\n",
        "    print(summary[i][1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYWSsLaAWXCX",
        "outputId": "11fa0585-b700-439e-af3c-2c979d6d043c"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Additionally, to strengthen long COVID research, future studies must account for biases and SARS-CoV-2 testing issues, build on viral-onset research, be inclusive of marginalized populations and meaningfully engage patients throughout the research process.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation- Rouge N1 & N2"
      ],
      "metadata": {
        "id": "3PXTf8sAhl0S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluation\n",
        "#Read reference summary\n",
        "ref_summary = '''Our infectious disease colleagues are adamant that restricting the movement of people into and around the hospital setting are effective clinical and epidemiological strategies that will help protect both the vulnerable patient population and health care providers themselves, \n",
        "who need to stay healthy so that they may care for their patients. In a health care institution, visitation restrictions not only affect inpatients but also have an impact on ambulatory patients who must come for diagnostic tests or interventions and who, if deprived access, might develop urgent or emergent conditions.\n",
        "Feedback should be sought from those individuals who would be affected by visitation restrictions, such as staff, patients and family members.Health care workers, being in direct communication with patients and families, bear the brunt of their anger and frustration regarding any restriction in visitation.\n",
        "If a family is allowed to visit a patient whose death is presumed to be imminent, then the patient's identity should be protected by using privacy strategies.\n",
        "'''\n",
        "summary = '''It could be argued that visitation restrictions, in light of a potential outbreak of a contagious disease, \n",
        "are ethically sound because of the compelling need to protect public health.However, even when public health concerns trump \n",
        "individual liberties, the ethical operationalization of this value would demand that 'those whose rights are being infringed' \n",
        "need to be managed in 'an ethical and even-handed manner so that they are not unfairly or disproportionately harmed by such \n",
        "measures' [1].This is an important and far-reaching consideration because SARS caused collateral damage and we know that the \n",
        "implementation of visitation restrictions will have an impact on a broad range of individuals.'''\n",
        "\n",
        "#The abstract is the target: Text containing the target (ground truth) text.- The Gold Standard\n",
        "# the summary generated is the prediction: Text containing the predicted text.\n",
        "\n",
        "!pip install -r rouge/requirements.txt\n",
        "!pip install rouge-score\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1','rouge2'], use_stemmer=True)\n",
        "scores = scorer.score(ref_summary,summary)\n",
        "                      \n",
        "scores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7yj6tTyWl2D",
        "outputId": "b2111fe7-40ef-4f41-ce46-c0e5c9b47925"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'rouge/requirements.txt'\u001b[0m\u001b[31m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: rouge-score in /usr/local/lib/python3.10/dist-packages (0.1.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge-score) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.22.4)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (4.65.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (8.1.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (2022.10.31)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (1.2.0)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'rouge1': Score(precision=0.44036697247706424, recall=0.2962962962962963, fmeasure=0.3542435424354244),\n",
              " 'rouge2': Score(precision=0.09259259259259259, recall=0.062111801242236024, fmeasure=0.07434944237918215)}"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, theres alot to explore before I settle on a single module for summarization.\n",
        "\n",
        "The recall value of the Rouge metric is what I am interested in, n gram value of 0.2 means that there is a 20% overlap in the unigrams of the summary generated and the written summary.\n",
        "theres a 6% overlap in the bigrams.I'd expect the bi gram value to be lower than the uni grams because the reference summary is of abstractive nature as it has been paraphrased."
      ],
      "metadata": {
        "id": "InxbKSDghxfz"
      }
    }
  ]
}